{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": "import os, sys\nsys.path.append('/root/catkin_ws/src/primitives/')\nimport pickle\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import axes3d\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport trimesh\nimport networkx\n\nfrom open3d import JVisualizer\n\nimport copy\nimport time\nimport argparse\nimport numpy as np\nfrom multiprocessing import Process, Pipe, Queue\nimport pickle\nimport rospy\nimport copy\nimport signal\nimport open3d\nfrom IPython import embed\n\nfrom yacs.config import CfgNode as CN\nfrom closed_loop_experiments import get_cfg_defaults\n\nfrom airobot import Robot\nfrom airobot.utils import pb_util\nfrom airobot.sensor.camera.rgbdcam_pybullet import RGBDCameraPybullet\nfrom airobot.utils import common\nimport pybullet as p\n\nfrom helper import util\nfrom macro_actions import ClosedLoopMacroActions, YumiGelslimPybulet\n# from closed_loop_eval import SingleArmPrimitives, DualArmPrimitives"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": "sys.path.append('/root/training/')\n\nimport os\nimport argparse\nimport time\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.autograd import Variable\nfrom data_loader import DataLoader\nfrom model import VAE\nfrom util import to_var, save_state, load_net_state, load_seed, load_opt_state\n\nimport scipy.signal as signal"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": "# with open ('/root/catkin_ws/src/primitives/data/grasp/face_ind_test_0/2.pkl', 'rb') as f:\n#     grasp_data = pickle.load(f)\n    \n# print(grasp_data.keys())"
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "/root/catkin_ws/src/primitives/data/pull/pull_face_all_0/test/275.pkl\n['goal', 'contact_obj_frame', 'contact_world_frame', 'transformation', 'start', 'result', 'keypoints_start', 'contact_pcd', 'keypoints_goal', 'obs']\n"
    }
   ],
   "source": "# with open ('/root/catkin_ws/src/primitives/data/pull/face_ind_large_0_fixed/1003.pkl', 'rb') as f:\n#     pull_data = pickle.load(f)\ngot_file = False\nwhile not got_file:\n    val = np.random.randint(low=0, high=5000)\n    test_path = '/root/catkin_ws/src/primitives/data/pull/pull_face_all_0/test/%d.pkl' % val\n#     test_path = '/root/catkin_ws/src/primitives/data/grasp/face_ind_test_0_fixed/%d.pkl' % val\n    if os.path.exists(test_path):\n        with open (test_path, 'rb') as f:\n            pull_data = pickle.load(f)\n#             grasp_data = pickle.load(f)\n            got_file = True\n\nprint(test_path)\nprint(pull_data.keys())\n# print(grasp_data.keys())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Training data: \n## Inputs: \nInitial observation, different representations:\n- ```'start'```: pose ```[x_pos, y_pos, z_pos, x_ori, y_ori, z_ori, w_ori]```\n- ```'keypoints_start'```: 3D location of box corners at start pose\n- ```'obs':'pcd_pts'```: Point cloud of box from 3 different viewpoints, with same global coordinate sys. Can be fused with np.concatenate (see below)\n\nGoal, different representations:\n- ```'goal'```: pose\n- ```'keypoints_goal'```: 3D location of box corners at goal pose"
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": "# start observation\nstart = pull_data['start']\nkeypoints_start = pull_data['keypoints_start']\npcd_pts = pull_data['obs']['pcd_pts']\npcd_pts_start = np.concatenate(pcd_pts, axis=0)\n\n# goal\ngoal = pull_data['goal']\nkeypoints_goal = pull_data['keypoints_goal']\n\n# # start observation\n# start = grasp_data['start']\n# keypoints_start = grasp_data['keypoints_start']\n# pcd_pts = grasp_data['obs']['pcd_pts']\n# pcd_pts_start = np.concatenate(pcd_pts, axis=0)\n\n# # goal\n# goal = grasp_data['goal']\n# keypoints_goal = grasp_data['keypoints_goal']"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Outputs:\n## Pulling/Pushing (single arm)\nRobot palm pose in the object frame, for active arm -- active arm currently based on which side of the table the object starts on. TODO perhaps includes predicting which arm is active, once we move to more diverse data. For now, everything for pulling happens with the right arm\n- ```'contact_obj_frame'```: pose ```[x_pos, y_pos, z_pos, x_ori, y_ori, z_ori, w_ori]```, specified with respect to the coordinate system located at the object center of mass at the start pose\n\n## Grasping/Pivoting (dual arm)\n\nRight and left robot palm pose in the object frame\n- ```'contact_obj_frame':'right'```: right palm pose ```[x_pos, y_pos, z_pos, x_ori, y_ori, z_ori, w_ori]```, specified with respect to the coordinate system located at the object center of mass at the start pose\n- ```'contact_obj_frame':'left'```: left palm pose ```[x_pos, y_pos, z_pos, x_ori, y_ori, z_ori, w_ori]```, specified with respect to the coordinate system located at the object center of mass at the start pose"
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": "# right palm contact for pulling\ncontact_r = pull_data['contact_obj_frame']['right']\n\n# both palms contact for grasping\n# contact_r = grasp_data['contact_obj_frame']['right']\n# contact_l = grasp_data['contact_obj_frame']['left']"
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": "def pose_to_list(palm_pose_tensor):\n    pos = palm_pose_tensor[:3].data.cpu().numpy()\n    ori = palm_pose_tensor[3:].data.cpu().numpy()\n    \n    ori = ori/np.linalg.norm(ori)\n    \n    return pos.tolist() + ori.tolist()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Testing"
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Encoder(\n  (hidden_layers): Sequential(\n    (0): Linear(in_features=38, out_features=128, bias=True)\n    (1): ReLU()\n  )\n  (mu_head): Sequential(\n    (0): Linear(in_features=128, out_features=3, bias=True)\n  )\n  (logvar_head): Sequential(\n    (0): Linear(in_features=128, out_features=3, bias=True)\n  )\n)\nDecoder(\n  (decoder): Sequential(\n    (0): Linear(in_features=34, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=128, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=128, out_features=128, bias=True)\n    (5): ReLU()\n  )\n  (right_head): Sequential(\n    (0): Linear(in_features=128, out_features=7, bias=True)\n  )\n  (left_head): Sequential(\n    (0): Linear(in_features=128, out_features=7, bias=True)\n  )\n)\n"
    }
   ],
   "source": "# load trained model\n\n# vae = VAE(14, 7, 2, 0.0003)\n# load_net_state(vae, '/root/training/saved_models/pose_init_small_batch_1/pose_init_small_batch_1_epoch_63.pt')\n\n# vae = VAE(31, 7, 3, 0.0003)\n# load_net_state(vae, '/root/training/saved_models/pull_keypoints_init_small_batch_0/pull_keypoints_init_small_batch_0_epoch_99.pt')\n\n# vae = VAE(14, 14, 3, 0.0003)\n# load_net_state(vae, '/root/training/saved_models/grasp_pose_init_small_batch_0/grasp_pose_init_small_batch_0_epoch_298.pt')\n\n# vae = VAE(31, 14, 3, 0.0003)\n# load_net_state(vae, '/root/training/saved_models/grasp_keypoints_init_small_batch_1/grasp_keypoints_init_small_batch_1_epoch_199.pt')\n\n# vae = VAE(31, 7, 3, 0.0003)\n# print(vae.encoder)\n# print(vae.decoder)\n# load_net_state(vae, '/root/training/saved_models/grasp_keypoints_two_heads_small_batch_r_only_0/grasp_keypoints_two_heads_small_batch_r_only_0_epoch_5.pt')\n\n# vae = VAE(31+14, 7, 3, 31, 0.0003)\n# print(vae.encoder)\n# print(vae.decoder)\n# load_net_state(vae, '/root/training/saved_models/grasp_keypoints_two_heads_full_data_grasp_input_1/grasp_keypoints_two_heads_full_data_grasp_input_1_epoch_199.pt')\n\nvae = VAE(24+7+7, 7, 3, 24+7, 0.0003)\nprint(vae.encoder)\nprint(vae.decoder)\nload_net_state(vae, '/root/training/saved_models/pull_keypoints_two_heads_diverse_goals_full_data_palm_input_0/pull_keypoints_two_heads_diverse_goals_full_data_palm_input_0_epoch_199.pt')"
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": "# sample from the latent space and use the decoder/generator to produce contacts, visualize below\n# x = torch.from_numpy(np.asarray(start+goal, dtype=np.float32))\n# x = torch.from_numpy(np.hstack((keypoints_start.flatten().astype(np.float32), np.asarray(goal, dtype=np.float32))))\n\nx = torch.from_numpy(np.hstack((keypoints_start.flatten().astype(np.float32), np.asarray(goal, dtype=np.float32))))\n\n# contact_input = torch.from_numpy(np.asarray(contact_r+contact_l, dtype=np.float32))\ncontact_input = torch.from_numpy(np.asarray(contact_r, dtype=np.float32))\n\ndecoder_input = x[:24+7]\nz, pose_sample, z_mu, z_logvar = output = vae.forward(torch.cat((x, contact_input), dim=0), decoder_input)"
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "mu: tensor([ 0.0041, -0.0007,  0.0026], grad_fn=<AddBackward0>), std: tensor([0.9995, 1.0003, 1.0021], grad_fn=<ExpBackward>)\n"
    }
   ],
   "source": "print(\"mu: \" + str(z_mu) + \", std: \" + str(torch.exp(0.5*z_logvar)))"
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(tensor([-0.0041,  0.0639,  0.0005,  0.0019,  0.0204, -0.0013, -0.0471],\n       grad_fn=<AddBackward0>), tensor([ 0.0678, -0.0204, -0.0199,  0.0190, -0.0372, -0.1007,  0.0174],\n       grad_fn=<AddBackward0>))\n"
    }
   ],
   "source": "# pose = vae.decode(z)\n# pose = vae.decode(z_mu)\n\nz_sample = torch.randn_like(torch.zeros(3, ))\n\npose = vae.decode(z_sample, decoder_input)\nprint(pose)"
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": "# pose_r = pose[:7]\n# pose_l = pose[7:]\n\npose_r = pose[0]\npose_l = pose[1]"
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": "pull_data_eval = copy.deepcopy(pull_data)\npull_data_eval['contact_obj_frame']['right'] = pose_to_list(pose_r)\n\n# grasp_data_eval = copy.deepcopy(grasp_data)\n# grasp_data_eval['contact_obj_frame']['right'] = pose_to_list(pose_r)\n# grasp_data_eval['contact_obj_frame']['left'] = pose_to_list(pose_l)"
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "data: [0.03209600577701585, 0.07145000249147417, 0.021195390451699506, -1.1169949666027594e-16, -0.40312516425753553, 3.6003622236378e-18, 0.9151448529835783]\n--------\neval: [-0.004069801419973373, 0.06393201649188995, 0.0005116462707519531, 0.037123922258615494, 0.396526038646698, -0.02600213512778282, -0.9169039130210876]\n"
    }
   ],
   "source": "print(\"data: \" + str(pull_data['contact_obj_frame']['right']))\nprint(\"--------\")\nprint(\"eval: \" + str(pull_data_eval['contact_obj_frame']['right']))\n\n# print(\"right: \" + str(grasp_data_eval['contact_obj_frame']['right']))\n# print(\"----------------------------\")\n# print(\"left: \" + str(grasp_data_eval['contact_obj_frame']['left']))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Visualization"
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Metadata keys: \n"
    }
   ],
   "source": "with open('/root/catkin_ws/src/primitives/data/pull/face_ind_large_0/metadata.pkl', 'rb') as mf:\n    metadata = pickle.load(mf)\n# with open('/root/catkin_ws/src/primitives/data/grasp/face_ind_test_0/metadata.pkl', 'rb') as mf:\n#     metadata = pickle.load(mf)\n\n\nprint('Metadata keys: ')\ndynamics_info = metadata['dynamics']\nmesh_file = metadata['mesh_file']\npalm_mesh_file = '/root/catkin_ws/src/config/descriptions/meshes/mpalm/mpalms_all_coarse.stl'\ntable_mesh_file = '/root/catkin_ws/src/config/descriptions/meshes/table/table_top.stl'\ncfg = metadata['cfg']"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Visualize contact on object"
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": "def vis_palms(data, name='pull'):\n    obj_mesh = trimesh.load_mesh(mesh_file)\n    r_palm_mesh = trimesh.load_mesh(palm_mesh_file)\n    l_palm_mesh = trimesh.load_mesh(palm_mesh_file)\n    table_mesh = trimesh.load_mesh(table_mesh_file)\n    \n    obj_pos_world = data['start'][:3]\n    obj_ori_world = data['start'][3:]\n    obj_ori_mat = common.quat2rot(obj_ori_world)\n    h_trans = np.zeros((4, 4))\n    h_trans[:3, :3] = obj_ori_mat\n    h_trans[:-1, -1] = obj_pos_world\n    h_trans[-1, -1] = 1\n\n    obj_mesh.apply_transform(h_trans)\n    if name == 'pull':\n        tip_contact_r_obj = util.list2pose_stamped(data['contact_obj_frame']['right'])\n        tip_contact_r = util.convert_reference_frame(\n            pose_source=tip_contact_r_obj,\n            pose_frame_target=util.unit_pose(),\n            pose_frame_source=util.list2pose_stamped(data['start']))\n\n        wrist_contact_r = util.convert_reference_frame(\n            pose_source=util.list2pose_stamped(cfg.TIP_TO_WRIST_TF),\n            pose_frame_target=util.unit_pose(),\n            pose_frame_source=tip_contact_r)\n\n        wrist_contact_r_list = util.pose_stamped2list(wrist_contact_r)\n        \n        palm_pos_world_r = wrist_contact_r_list[:3]\n        palm_ori_world_r = wrist_contact_r_list[3:]\n        palm_ori_mat = common.quat2rot(palm_ori_world_r)\n        h_trans = np.zeros((4, 4))\n        h_trans[:3, :3] = palm_ori_mat\n        h_trans[:-1, -1] = palm_pos_world_r\n        h_trans[-1, -1] = 1\n\n        r_palm_mesh.apply_transform(h_trans)      \n        \n        scene = trimesh.Scene([obj_mesh, r_palm_mesh, table_mesh])        \n    else:\n        tip_contact_r_obj = util.list2pose_stamped(data['contact_obj_frame']['right'])\n        tip_contact_l_obj = util.list2pose_stamped(data['contact_obj_frame']['left'])\n\n        tip_contact_r = util.convert_reference_frame(\n            pose_source=tip_contact_r_obj,\n            pose_frame_target=util.unit_pose(),\n            pose_frame_source=util.list2pose_stamped(data['start']))\n            \n        tip_contact_l = util.convert_reference_frame(\n            pose_source=tip_contact_l_obj,\n            pose_frame_target=util.unit_pose(),\n            pose_frame_source=util.list2pose_stamped(data['start']))            \n            \n        wrist_contact_r = util.convert_reference_frame(\n            pose_source=util.list2pose_stamped(cfg.TIP_TO_WRIST_TF),\n            pose_frame_target=util.unit_pose(),\n            pose_frame_source=tip_contact_r)\n\n        wrist_contact_l = util.convert_reference_frame(\n            pose_source=util.list2pose_stamped(cfg.TIP_TO_WRIST_TF),\n            pose_frame_target=util.unit_pose(),\n            pose_frame_source=tip_contact_l)\n\n        wrist_contact_r_list = util.pose_stamped2list(wrist_contact_r)\n        wrist_contact_l_list = util.pose_stamped2list(wrist_contact_l)\n        \n        palm_pos_world_r = wrist_contact_r_list[:3]\n        palm_ori_world_r = wrist_contact_r_list[3:]\n        palm_ori_mat = common.quat2rot(palm_ori_world_r)\n        h_trans = np.zeros((4, 4))\n        h_trans[:3, :3] = palm_ori_mat\n        h_trans[:-1, -1] = palm_pos_world_r\n        h_trans[-1, -1] = 1\n\n        r_palm_mesh.apply_transform(h_trans)\n        \n        palm_pos_world_l = wrist_contact_l_list[:3]\n        palm_ori_world_l = wrist_contact_l_list[3:]\n        palm_ori_mat = common.quat2rot(palm_ori_world_l)\n        h_trans = np.zeros((4, 4))\n        h_trans[:3, :3] = palm_ori_mat\n        h_trans[:-1, -1] = palm_pos_world_l\n        h_trans[-1, -1] = 1\n\n        l_palm_mesh.apply_transform(h_trans)        \n        \n\n        scene = trimesh.Scene([obj_mesh, r_palm_mesh, l_palm_mesh, table_mesh])\n#         scene = trimesh.Scene([obj_mesh, r_palm_mesh, table_mesh])\n    good_camera_euler = [1.0513555,  -0.02236318, -1.62958927]\n    for key in scene.geometry.keys():\n        print(key)\n        if 'mpalms_all_coarse' in key:\n            scene.geometry[key].visual.face_colors = [100, 100, 0, 30]\n\n    scene.geometry['table_top.stl'].visual.face_colors = [200, 200, 200, 250]\n    scene.geometry['realsense_box_experiments.stl'].visual.face_colors = [200, 200, 250, 250]\n    scene.set_camera(angles=good_camera_euler, center=data['start'][:3], distance=0.8)\n    return scene"
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": "def correct_pos(data):\n    contact_obj_frame_pred = util.list2pose_stamped(pull_data_eval['contact_obj_frame'])\n    contact_world_frame = util.convert_reference_frame(contact_obj_frame_pred, util.unit_pose(), util.list2pose_stamped(start))\n    contact_pos = open3d.utility.DoubleVector(np.array(util.pose_stamped2list(contact_world_frame)[:3]))\n\n    pcd = open3d.geometry.PointCloud()\n    pcd.points = open3d.utility.Vector3dVector(np.concatenate(pull_data_eval['obs']['pcd_pts']))\n    pcd.colors = open3d.utility.Vector3dVector(np.concatenate(pull_data_eval['obs']['pcd_colors']) / 255.0)\n\n    kdtree = open3d.geometry.KDTreeFlann(pcd)\n    nearest_pt_ind = kdtree.search_knn_vector_3d(contact_pos, 1)[1][0]\n    nearest_pt_world = np.asarray(pcd.points)[nearest_pt_ind]\n\n    contact_world_frame_corrected = copy.deepcopy(contact_world_frame)\n    contact_world_frame_corrected.pose.position.x = nearest_pt_world[0]\n    contact_world_frame_corrected.pose.position.y = nearest_pt_world[1]\n    contact_world_frame_corrected.pose.position.z = nearest_pt_world[2]\n\n    contact_obj_frame_corrected = util.pose_stamped2list(util.convert_reference_frame(contact_world_frame_corrected, util.list2pose_stamped(start), util.unit_pose()))\n    new_data = copy.deepcopy(data)\n    new_data['contact_obj_frame'] = contact_obj_frame_corrected\n    return new_data"
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "realsense_box_experiments.stl\nmpalms_all_coarse.stl\ntable_top.stl\n"
    },
    {
     "data": {
      "text/plain": "SceneViewer(width=1800, height=1350)"
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "# scene = vis_palms(grasp_data, name='grasp')\nscene = vis_palms(pull_data, name='pull')\n\nscene.show(viewer='gl')"
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "realsense_box_experiments.stl\nmpalms_all_coarse.stl\ntable_top.stl\n"
    },
    {
     "data": {
      "text/plain": "SceneViewer(width=1800, height=1350)"
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "z_sample = torch.randn_like(torch.zeros(3, ))\n\nx = torch.from_numpy(np.hstack((keypoints_start.flatten().astype(np.float32), np.asarray(goal, dtype=np.float32))))\ndecoder_input = x[:31]\n\npose = vae.decode(z_sample, decoder_input)\npose_r = pose[0]\npose_l = pose[1]\n\npull_data_eval = copy.deepcopy(pull_data)\npull_data_eval['contact_obj_frame']['right'] = pose_to_list(pose_r)\npull_data_eval['contact_obj_frame']['left'] = pose_to_list(pose_l)\n\nscene = vis_palms(pull_data_eval, name='pull')\n\n# grasp_data_eval = copy.deepcopy(grasp_data)\n# grasp_data_eval['contact_obj_frame']['right'] = pose_to_list(pose_r)\n# grasp_data_eval['contact_obj_frame']['left'] = pose_to_list(pose_l)\n\n# scene = vis_palms(grasp_data_eval, name='grasp')\nscene.show(viewer='gl')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# scene = vis_palms(pull_data_eval, name='pull')\nscene = vis_palms(grasp_data_eval, name='grasp')\n\nscene.show(viewer='gl')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# scene = vis_palms(correct_pos(pull_data_eval), name='pull')\n# scene = vis_palms(grasp_data, name='grasp')\n\n# scene.show(viewer='gl')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# x = torch.from_numpy(np.asarray(start+goal, dtype=np.float32))\n# output = vae.forward(x)\n\n# pos = output[1][:3].data.cpu().numpy()\n# ori = output[1][3:].data.cpu().numpy()\n\n# ori = ori/np.linalg.norm(ori)\n\n# pull_data_eval = copy.deepcopy(pull_data)\n# pull_data_eval['contact_obj_frame'] = pos.tolist() + ori.tolist()\n\n# scene = vis_palms(pull_data_eval, name='pull')\n\n# x = torch.from_numpy(np.hstack((keypoints_start.flatten().astype(np.float32), np.asarray(goal, dtype=np.float32))))\n# output = vae.forward(x)\n\n# pose_r = output[1][:7]\n# pose_l = output[1][7:]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "outputs = []\nz = torch.randn_like(torch.zeros(500, 2))\ndecoder_inputs = decoder_input.repeat(500).reshape(500, decoder_input.shape[0])\n\noutput = vae.decode(z, decoder_inputs)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# output_mean = []\n# output_std = []\n# for i in range(output[0].shape[1]):\n#     avg_dim_i = torch.mean(output[0][:, i], axis=0)\n#     std_dim_i = torch.std(output[0][:, i], axis=0)\n#     print(avg_dim_i)\n#     output_mean.append(avg_dim_i.data.cpu())\n#     output_std.append(std_dim_i.data.cpu())\n\noutput_mean = []\noutput_std = []\nfor i in range(output[0].shape[1]):\n    avg_dim_i_r = torch.mean(output[0][:, i], axis=0)\n    std_dim_i_r = torch.std(output[0][:, i], axis=0)\n    \n    avg_dim_i_l = torch.mean(output[1][:, i], axis=0)\n    std_dim_i_l = torch.std(output[1][:, i], axis=0)    \n    print(avg_dim_i)\n    output_mean.append((avg_dim_i_r.data.cpu(), avg_dim_i_l.data.cpu()))\n    output_std.append((std_dim_i_r.data.cpu(), avg_dim_i_l.data.cpu()))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# pos = np.asarray(output_mean[:3])\n# ori = np.asarray(output_mean[3:])\n\n# ori = ori/np.linalg.norm(ori)\n\n# pull_data_eval = copy.deepcopy(pull_data)\n# pull_data_eval['contact_obj_frame'] = pos.tolist() + ori.tolist()\n\n# scene = vis_palms(pull_data_eval, name='pull')\n\n# pose_r = output_mean[:7]\n# pose_l = output_mean[7:]\n\n# pose_r = output_mean[0][:7]\n# pose_l = output_mean[1][7:]\n\npose_r, pose_l = [], []\nfor i in range(output[0].shape[1]):\n    pose_r.append(output_mean[i][0])\n    pose_l.append(output_mean[i][1])\n\ngrasp_data_eval = copy.deepcopy(grasp_data)\ngrasp_data_eval['contact_obj_frame']['right'] = pose_r\ngrasp_data_eval['contact_obj_frame']['left'] = pose_l\nscene = vis_palms(grasp_data_eval, name='grasp')\n\nscene.show(viewer='gl')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "loss_data = np.load('/root/training/saved_models/pull_keypoints_two_heads_diverse_goals_full_data_palm_input_0/pull_keypoints_two_heads_diverse_goals_full_data_palm_input_0_epoch_89_recon_loss.npz', allow_pickle=True)\nloss_data.keys()\nloss_data['pos_loss'].shape"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plt.figure()\nplt.plot(loss_data['pos_loss'])\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plt.figure()\nplt.plot(loss_data['ori_loss'])\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# sos = signal.butter(3, 0.03, 'low', output='sos')\nsos = signal.butter(3, 0.3, 'low', output='sos')\nfiltered = signal.sosfilt(sos, loss_data['ori_loss'])\n\nplt.figure()\nplt.plot(filtered[4:])\nplt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
