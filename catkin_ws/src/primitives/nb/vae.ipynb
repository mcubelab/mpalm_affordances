{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, sys\nsys.path.append('/root/catkin_ws/src/primitives/')\nimport pickle\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import axes3d\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport trimesh\nimport networkx\n\nfrom open3d import JVisualizer\n\nimport copy\nimport time\nimport argparse\nimport numpy as np\nfrom multiprocessing import Process, Pipe, Queue\nimport pickle\nimport rospy\nimport copy\nimport signal\nimport open3d\nfrom IPython import embed\n\nfrom yacs.config import CfgNode as CN\nfrom closed_loop_experiments import get_cfg_defaults\n\nfrom airobot import Robot\nfrom airobot.utils import pb_util\nfrom airobot.sensor.camera.rgbdcam_pybullet import RGBDCameraPybullet\nfrom airobot.utils import common\nimport pybullet as p\n\nfrom helper import util\nfrom macro_actions import ClosedLoopMacroActions, YumiGelslimPybulet\n# from closed_loop_eval import SingleArmPrimitives, DualArmPrimitives"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "sys.path.append('/root/training/')\n\nimport os\nimport argparse\nimport time\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.autograd import Variable\nfrom data_loader import DataLoader\nfrom model import VAE, GoalVAE\nfrom util import to_var, save_state, load_net_state, load_seed, load_opt_state\n\nimport scipy.signal as signal\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.manifold import TSNE"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# with open ('/root/catkin_ws/src/primitives/data/grasp/face_ind_test_0/2.pkl', 'rb') as f:\n#     grasp_data = pickle.load(f)\n    \n# print(grasp_data.keys())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# with open ('/root/catkin_ws/src/primitives/data/pull/face_ind_large_0_fixed/1003.pkl', 'rb') as f:\n#     pull_data = pickle.load(f)\ndef get_rand_data(primitive='pull', train_test='test'):\n    got_file = False\n    while not got_file:\n        val = np.random.randint(low=0, high=5000)\n        if primitive == 'pull':\n            test_path = '/root/catkin_ws/src/primitives/data/pull/pull_face_all_0/%s/%d/pkl/%d.pkl' % (train_test, val, val)\n        elif primitive == 'grasp':\n            test_path = '/root/catkin_ws/src/primitives/data/grasp/face_ind_test_0_fixed/%s/%d/pkl/%d.pkl' % (train_test, val, val)\n        if os.path.exists(test_path):\n            with open (test_path, 'rb') as f:\n    #             pull_data = pickle.load(f)\n                data = pickle.load(f)\n                got_file = True\n\n    print(test_path)\n    # print(pull_data.keys())\n    print(data.keys())\n    return data\npull_data = get_rand_data(primitive='pull')\ngrasp_data = get_rand_data(primitive='grasp')\n\nwith open('/root/catkin_ws/src/primitives/data/pull/pull_face_all_0/metadata.pkl', 'rb') as mf:\n    metadata = pickle.load(mf)\n# with open('/root/catkin_ws/src/primitives/data/grasp/face_ind_test_0_fixed/metadata.pkl', 'rb') as mf:\n#     metadata = pickle.load(mf)\n\n\nprint('Metadata keys: ')\ndynamics_info = metadata['dynamics']\nmesh_file = metadata['mesh_file']\npalm_mesh_file = '/root/catkin_ws/src/config/descriptions/meshes/mpalm/mpalms_all_coarse.stl'\ntable_mesh_file = '/root/catkin_ws/src/config/descriptions/meshes/table/table_top.stl'\ncfg = metadata['cfg']"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Viz Utils"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def vis_palms(data, name='pull', goal='transformation', goal_number=1, palm_number=1):\n    obj_mesh = trimesh.load_mesh(mesh_file)\n    table_mesh = trimesh.load_mesh(table_mesh_file)\n    \n    obj_pos_world = data['start'][:3]\n    obj_ori_world = data['start'][3:]\n    obj_ori_mat = common.quat2rot(obj_ori_world)\n    h_trans = np.zeros((4, 4))\n    h_trans[:3, :3] = obj_ori_mat\n    h_trans[:-1, -1] = obj_pos_world\n    h_trans[-1, -1] = 1\n\n    obj_mesh.apply_transform(h_trans)\n    \n    if len(data['transformation_corrected']) > 0:\n        if not isinstance(data['transformation_corrected'][0], list):\n            tmp = data['transformation_corrected']\n            data['transformation_corrected'] = [tmp]\n    else:\n        goal_number = 0\n    if not isinstance(data['contact_obj_frame']['right'][0], list):\n        tmp_r = data['contact_obj_frame']['right']\n        tmp_l = data['contact_obj_frame']['left']\n        data['contact_obj_frame']['right'] = [tmp_r]\n        data['contact_obj_frame']['left'] = [tmp_l]\n        \n    \n    goal_obj_meshes = []\n    for i in range(goal_number):\n        goal_obj_mesh = trimesh.load_mesh(mesh_file)        \n        if goal == 'transformation':\n            T_mat = util.matrix_from_pose(util.list2pose_stamped(data['transformation_corrected'][i]))\n            T_start = util.matrix_from_pose(util.list2pose_stamped(data['start']))\n            T_goal = np.matmul(T_mat, T_start)\n            goal_obj_pose_world = util.pose_stamped2list(util.pose_from_matrix(T_goal))\n    #         goal_obj_pose_start = util.list2pose_stamped(data['transformation_corrected'])\n    #         goal_obj_pose_world = util.pose_stamped2list(\n    #             util.convert_reference_frame(\n    #                 goal_obj_pose_start,\n    #                 util.unit_pose(),\n    #                 util.list2pose_stamped(data['start'])))\n            goal_obj_pos_world = goal_obj_pose_world[:3]\n            goal_obj_ori_world = goal_obj_pose_world[3:]\n        else:\n            goal_obj_pos_world = data['goal'][:3]\n            goal_obj_ori_world = data['goal'][3:]\n        goal_obj_ori_mat = common.quat2rot(goal_obj_ori_world)\n        goal_h_trans = np.zeros((4, 4))\n        goal_h_trans[:3, :3] = goal_obj_ori_mat\n        goal_h_trans[:-1, -1] = goal_obj_pos_world\n        goal_h_trans[-1, -1] = 1 \n\n        goal_obj_mesh.apply_transform(goal_h_trans)\n        goal_obj_meshes.append(goal_obj_mesh)\n    \n    r_palm_meshes = []\n    l_palm_meshes = []\n    \n    if name == 'pull':\n        for j in range(palm_number):\n            r_palm_mesh = trimesh.load_mesh(palm_mesh_file)\n            l_palm_mesh = trimesh.load_mesh(palm_mesh_file)\n            \n            tip_contact_r_obj = util.list2pose_stamped(data['contact_obj_frame']['right'][j])\n            tip_contact_r = util.convert_reference_frame(\n                pose_source=tip_contact_r_obj,\n                pose_frame_target=util.unit_pose(),\n                pose_frame_source=util.list2pose_stamped(data['start']))\n\n            wrist_contact_r = util.convert_reference_frame(\n                pose_source=util.list2pose_stamped(cfg.TIP_TO_WRIST_TF),\n                pose_frame_target=util.unit_pose(),\n                pose_frame_source=tip_contact_r)\n\n            wrist_contact_r_list = util.pose_stamped2list(wrist_contact_r)\n\n            palm_pos_world_r = wrist_contact_r_list[:3]\n            palm_ori_world_r = wrist_contact_r_list[3:]\n            palm_ori_mat = common.quat2rot(palm_ori_world_r)\n            h_trans = np.zeros((4, 4))\n            h_trans[:3, :3] = palm_ori_mat\n            h_trans[:-1, -1] = palm_pos_world_r\n            h_trans[-1, -1] = 1\n\n            r_palm_mesh.apply_transform(h_trans)\n            r_palm_meshes.append(r_palm_mesh)\n        \n#         scene = trimesh.Scene([obj_mesh, r_palm_mesh, table_mesh, goal_obj_mesh]) \n#         scene = trimesh.Scene([obj_mesh, r_palm_mesh, table_mesh] + goal_obj_meshes)\n        scene = trimesh.Scene([obj_mesh, table_mesh] + r_palm_meshes + goal_obj_meshes)\n    else:\n        for j in range(palm_number):\n            r_palm_mesh = trimesh.load_mesh(palm_mesh_file)\n            l_palm_mesh = trimesh.load_mesh(palm_mesh_file)\n            \n            tip_contact_r_obj = util.list2pose_stamped(data['contact_obj_frame']['right'][j])\n            tip_contact_l_obj = util.list2pose_stamped(data['contact_obj_frame']['left'][j])\n\n            tip_contact_r = util.convert_reference_frame(\n                pose_source=tip_contact_r_obj,\n                pose_frame_target=util.unit_pose(),\n                pose_frame_source=util.list2pose_stamped(data['start']))\n\n            tip_contact_l = util.convert_reference_frame(\n                pose_source=tip_contact_l_obj,\n                pose_frame_target=util.unit_pose(),\n                pose_frame_source=util.list2pose_stamped(data['start']))            \n\n            wrist_contact_r = util.convert_reference_frame(\n                pose_source=util.list2pose_stamped(cfg.TIP_TO_WRIST_TF),\n                pose_frame_target=util.unit_pose(),\n                pose_frame_source=tip_contact_r)\n\n            wrist_contact_l = util.convert_reference_frame(\n                pose_source=util.list2pose_stamped(cfg.TIP_TO_WRIST_TF),\n                pose_frame_target=util.unit_pose(),\n                pose_frame_source=tip_contact_l)\n\n            wrist_contact_r_list = util.pose_stamped2list(wrist_contact_r)\n            wrist_contact_l_list = util.pose_stamped2list(wrist_contact_l)\n\n            palm_pos_world_r = wrist_contact_r_list[:3]\n            palm_ori_world_r = wrist_contact_r_list[3:]\n            palm_ori_mat = common.quat2rot(palm_ori_world_r)\n            h_trans = np.zeros((4, 4))\n            h_trans[:3, :3] = palm_ori_mat\n            h_trans[:-1, -1] = palm_pos_world_r\n            h_trans[-1, -1] = 1\n\n            r_palm_mesh.apply_transform(h_trans)\n\n            palm_pos_world_l = wrist_contact_l_list[:3]\n            palm_ori_world_l = wrist_contact_l_list[3:]\n            palm_ori_mat = common.quat2rot(palm_ori_world_l)\n            h_trans = np.zeros((4, 4))\n            h_trans[:3, :3] = palm_ori_mat\n            h_trans[:-1, -1] = palm_pos_world_l\n            h_trans[-1, -1] = 1\n\n            l_palm_mesh.apply_transform(h_trans)        \n\n            r_palm_meshes.append(r_palm_mesh)\n            l_palm_meshes.append(l_palm_mesh)\n\n#         scene = trimesh.Scene([obj_mesh, r_palm_mesh, l_palm_mesh, table_mesh, goal_obj_mesh])\n#         scene = trimesh.Scene([obj_mesh, r_palm_mesh, l_palm_mesh, table_mesh] + goal_obj_meshes)\n        scene = trimesh.Scene([obj_mesh, table_mesh] + r_palm_meshes + l_palm_meshes + goal_obj_meshes)\n#         scene = trimesh.Scene([obj_mesh, r_palm_mesh, table_mesh])\n    good_camera_euler = [1.0513555,  -0.02236318, -1.62958927]\n    box_count = 0\n    for key in scene.geometry.keys():\n        print(key)\n        if 'mpalms_all_coarse' in key:\n            scene.geometry[key].visual.face_colors = [100, 100, 0, 30]\n        if 'realsense_box_experiments' in key:\n            if box_count == 0:\n                scene.geometry[key].visual.face_colors = [250, 200, 200, 150]\n                box_count += 1\n            else:\n                scene.geometry[key].visual.face_colors = [200, 200, 250, 150]                \n\n    scene.geometry['table_top.stl'].visual.face_colors = [200, 200, 200, 250]\n#     scene.geometry['realsense_box_experiments.stl'].visual.face_colors = [200, 200, 250, 250]\n    scene.set_camera(angles=good_camera_euler, center=data['start'][:3], distance=0.8)\n    return scene\n\ndef correct_pos(data):\n    contact_obj_frame_pred = util.list2pose_stamped(pull_data_eval['contact_obj_frame'])\n    contact_world_frame = util.convert_reference_frame(contact_obj_frame_pred, util.unit_pose(), util.list2pose_stamped(start))\n    contact_pos = open3d.utility.DoubleVector(np.array(util.pose_stamped2list(contact_world_frame)[:3]))\n\n    pcd = open3d.geometry.PointCloud()\n    pcd.points = open3d.utility.Vector3dVector(np.concatenate(pull_data_eval['obs']['pcd_pts']))\n    pcd.colors = open3d.utility.Vector3dVector(np.concatenate(pull_data_eval['obs']['pcd_colors']) / 255.0)\n\n    kdtree = open3d.geometry.KDTreeFlann(pcd)\n    nearest_pt_ind = kdtree.search_knn_vector_3d(contact_pos, 1)[1][0]\n    nearest_pt_world = np.asarray(pcd.points)[nearest_pt_ind]\n\n    contact_world_frame_corrected = copy.deepcopy(contact_world_frame)\n    contact_world_frame_corrected.pose.position.x = nearest_pt_world[0]\n    contact_world_frame_corrected.pose.position.y = nearest_pt_world[1]\n    contact_world_frame_corrected.pose.position.z = nearest_pt_world[2]\n\n    contact_obj_frame_corrected = util.pose_stamped2list(util.convert_reference_frame(contact_world_frame_corrected, util.list2pose_stamped(start), util.unit_pose()))\n    new_data = copy.deepcopy(data)\n    new_data['contact_obj_frame'] = contact_obj_frame_corrected\n    return new_data\n\n\ndef plot_keypoint_goal(data):\n\n    i_list=[0, 1, 4, 5, 2, 3, 0, 5, 1, 1, 0, 2]\n    j_list=[1, 2, 5, 6, 3, 6, 1, 1, 5, 3, 4, 4]\n    k_list=[2, 3, 6, 7, 6, 7, 4, 4, 7, 7, 6, 6]\n    \n    red_marker = {\n        'size' : 1.0,\n        'color' : 'red',                # set color to an array/list of desired values\n        'colorscale' : 'Viridis',   # choose a colorscale\n        'opacity' : 0.8\n    } \n    blue_marker = {\n        'size' : 1.0,\n        'color' : 'blue',                # set color to an array/list of desired values\n        'colorscale' : 'Viridis',   # choose a colorscale\n        'opacity' : 0.8\n    }   \n    \n    black_marker = {\n        'size' : 3.0,\n        'color' : 'black',                # set color to an array/list of desired values\n        'colorscale' : 'Viridis',   # choose a colorscale\n        'opacity' : 0.8\n    }       \n    \n    kp_start = data['keypoints_start']\n\n    start_data = {\n        'type': 'mesh3d',\n        'x': kp_start[:, 0],\n        'y': kp_start[:, 1],\n        'z': kp_start[:, 2],\n        'color': 'red',\n        'opacity': 0.95,\n        'delaunayaxis': 'z',\n        'i': i_list,\n        'j': j_list,\n        'k': k_list,\n        'flatshading': True\n    }\n    \n    start_kp_data = {\n        'type': 'scatter3d',\n        'x': kp_start[:, 0],\n        'y': kp_start[:, 1],\n        'z': kp_start[:, 2],\n        'mode': 'markers',\n        'marker': red_marker        \n    }\n    \n    plane_data = {\n        'type': 'mesh3d',\n        'x': [-1, 1, 1, -1],\n        'y': [-1, -1, 1, 1],\n        'z': [0, 0, 0, 0],\n        'color': 'gray',\n        'opacity': 0.5,\n        'delaunayaxis': 'z'\n    }\n    \n    fig_data = []   \n    fig_data.append(start_data)    \n    fig_data.append(plane_data)\n    \n    \n    if isinstance(data['keypoints_goal_corrected'], list):        \n        for kp_goal in data['keypoints_goal_corrected']:\n            goal_data = {\n                'type': 'mesh3d',\n                'x': kp_goal[:, 0],\n                'y': kp_goal[:, 1],\n                'z': kp_goal[:, 2],\n                'color': 'blue',\n                'opacity': 0.05,\n                'delaunayaxis': 'z',\n                'i': i_list,\n                'j': j_list,\n                'k': k_list,\n                'flatshading': True\n            }\n            \n            goal_kp_data = {\n                'type': 'scatter3d',\n                'x': kp_goal[:, 0],\n                'y': kp_goal[:, 1],\n                'z': kp_goal[:, 2],\n                'mode': 'markers',\n                'marker': black_marker\n            }\n            \n            fig_data.append(goal_data)\n            fig_data.append(goal_kp_data)\n    else:\n        kp_goal = data['keypoints_goal_corrected']\n        goal_data = {\n            'type': 'mesh3d',\n            'x': kp_goal[:, 0],\n            'y': kp_goal[:, 1],\n            'z': kp_goal[:, 2],\n            'color': 'blue',\n            'opacity': 0.05,\n            'delaunayaxis': 'z',\n            'i': i_list,\n            'j': j_list,\n            'k': k_list,\n            'flatshading': True\n        }\n        \n        goal_kp_data = {\n            'type': 'scatter3d',\n            'x': kp_goal[:, 0],\n            'y': kp_goal[:, 1],\n            'z': kp_goal[:, 2],\n            'mode': 'markers',\n            'marker': black_marker\n        }       \n        \n        fig_data.append(goal_data)\n        fig_data.append(goal_kp_data)\n\n    fig = go.Figure(data=fig_data)\n    \n#     camera = {\n#         'up': {'x': 0, 'y': 0,'z': 1},\n#         'center': {'x': data['start'][0], 'y': data['start'][1], 'z': data['start'][2]-0.5},\n#         'eye': {'x': -1.0, 'y': 0.0, 'z': 0.01}\n#     } \n    camera = {\n        'up': {'x': 0, 'y': 0,'z': 1},\n        'center': {'x': 0.45, 'y': 0, 'z': 0.0},\n        'eye': {'x': -1.0, 'y': 0.0, 'z': 0.01}\n    } \n\n    scene = {\n        'xaxis': {'nticks': 10, 'range': [-0.1, 0.9]},\n        'yaxis': {'nticks': 16, 'range': [-0.5, 0.5]},\n        'zaxis': {'nticks': 8, 'range': [-0.01, 0.99]}\n    }\n    width = 700\n    margin = {'r': 20, 'l': 10, 'b': 10, 't': 10}    \n    \n    fig.update_layout(\n        scene=scene, \n        scene_camera=camera,\n        width=width,\n        margin=margin\n    )    \n    return fig\n\ndef plot_keypoint_plan(keypoint_plan):\n\n    i_list=[0, 1, 4, 5, 2, 3, 0, 5, 1, 1, 0, 2]\n    j_list=[1, 2, 5, 6, 3, 6, 1, 1, 5, 3, 4, 4]\n    k_list=[2, 3, 6, 7, 6, 7, 4, 4, 7, 7, 6, 6]\n    \n    red_marker = {\n        'size' : 1.0,\n        'color' : 'red',                # set color to an array/list of desired values\n        'colorscale' : 'Viridis',   # choose a colorscale\n        'opacity' : 0.8\n    } \n    blue_marker = {\n        'size' : 1.0,\n        'color' : 'blue',                # set color to an array/list of desired values\n        'colorscale' : 'Viridis',   # choose a colorscale\n        'opacity' : 0.8\n    }   \n    \n    black_marker = {\n        'size' : 3.0,\n        'color' : 'black',                # set color to an array/list of desired values\n        'colorscale' : 'Viridis',   # choose a colorscale\n        'opacity' : 0.8\n    }       \n    \n    kp_start = keypoint_plan[0]['keypoints_start']\n\n    start_data = {\n        'type': 'mesh3d',\n        'x': kp_start[:, 0],\n        'y': kp_start[:, 1],\n        'z': kp_start[:, 2],\n        'color': 'red',\n        'opacity': 0.95,\n        'delaunayaxis': 'z',\n        'i': i_list,\n        'j': j_list,\n        'k': k_list,\n        'flatshading': True\n    }\n    \n    start_kp_data = {\n        'type': 'scatter3d',\n        'x': kp_start[:, 0],\n        'y': kp_start[:, 1],\n        'z': kp_start[:, 2],\n        'mode': 'markers',\n        'marker': red_marker        \n    }\n    \n    plane_data = {\n        'type': 'mesh3d',\n        'x': [-1, 1, 1, -1],\n        'y': [-1, -1, 1, 1],\n        'z': [0, 0, 0, 0],\n        'color': 'gray',\n        'opacity': 0.5,\n        'delaunayaxis': 'z'\n    }\n    \n    fig_data = []   \n    fig_data.append(start_data)    \n    fig_data.append(plane_data)\n    \n    for i in range(len(keypoint_plan) - 1):\n        kp_goal = keypoint_plan[i+1]['keypoints_start']\n        \n        goal_data = {\n            'type': 'mesh3d',\n            'x': kp_goal[:, 0],\n            'y': kp_goal[:, 1],\n            'z': kp_goal[:, 2],\n            'color': 'blue',\n            'opacity': 0.05,\n            'delaunayaxis': 'z',\n            'i': i_list,\n            'j': j_list,\n            'k': k_list,\n            'flatshading': True\n        }\n        goal_kp_data = {\n            'type': 'scatter3d',\n            'x': kp_goal[:, 0],\n            'y': kp_goal[:, 1],\n            'z': kp_goal[:, 2],\n            'mode': 'markers',\n            'marker': black_marker\n        }\n        fig_data.append(goal_data)\n        fig_data.append(goal_kp_data)\n    \n    fig = go.Figure(data=fig_data)\n    \n    camera = {\n        'up': {'x': 0, 'y': 0,'z': 1},\n        'center': {'x': 0.45, 'y': 0, 'z': 0.0},\n        'eye': {'x': -1.0, 'y': 0.0, 'z': 0.01}\n    }    \n\n    scene = {\n        'xaxis': {'nticks': 10, 'range': [-0.1, 0.9]},\n        'yaxis': {'nticks': 16, 'range': [-0.5, 0.5]},\n        'zaxis': {'nticks': 8, 'range': [-0.01, 0.99]}\n    }\n    width = 700\n    margin = {'r': 20, 'l': 10, 'b': 10, 't': 10}    \n    \n    fig.update_layout(\n        scene=scene, \n        scene_camera=camera,\n        width=width,\n        margin=margin\n    )    \n    return fig\n\ndef transform_keypoints(data, number=1):\n    if not isinstance(data['transformation_corrected'][0], list):\n        tmp = data['transformation_corrected']\n        data['transformation_corrected'] = [tmp]\n    \n    data['keypoints_goal_corrected'] = []\n    \n    keypoints_start_homog = np.hstack(\n        (data['keypoints_start'], np.ones(\n            (data['keypoints_start'].shape[0], 1)))\n    )    \n    \n    for i in range(number):\n        T_mat = util.matrix_from_pose(util.list2pose_stamped(data['transformation_corrected'][i]))\n        data['keypoints_goal_corrected'].append(np.matmul(T_mat, keypoints_start_homog.T).T[:, :3])\n    return data\n\ndef transform_keypoints_full(data, number=1):\n    if not isinstance(data['transformation_corrected'][0], list):\n        tmp = data['transformation_corrected']\n        data['transformation_corrected'] = [tmp]\n    \n    data['keypoints_goal_corrected'] = []\n    data['goal'] = []\n    \n    keypoints_start_homog = np.hstack(\n        (data['keypoints_start'], np.ones(\n            (data['keypoints_start'].shape[0], 1)))\n    )\n    \n    T_start = util.matrix_from_pose(util.list2pose_stamped(data['start']))\n    \n    for i in range(number):\n        T_mat = util.matrix_from_pose(util.list2pose_stamped(data['transformation_corrected'][i]))\n        data['keypoints_goal_corrected'].append(np.matmul(T_mat, keypoints_start_homog.T).T[:, :3])\n        T_goal = np.matmul(T_mat, T_start)        \n        data['goal'].append(util.pose_stamped2list(util.pose_from_matrix(T_goal)))\n    return data\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# scene = vis_palms(grasp_data, name='grasp', goal='goal')\nscene = vis_palms(get_rand_data('grasp'), name='grasp', goal='transformation')\n\nscene.show(viewer='gl')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Torch Utils"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def torch_pose_to_list(palm_pose_tensor):\n    pos = palm_pose_tensor[:3].data.cpu().numpy()\n    ori = palm_pose_tensor[3:].data.cpu().numpy()\n    \n    ori = ori/np.linalg.norm(ori)\n    \n    return pos.tolist() + ori.tolist()\n\ndef to_torch_contact(data):\n    inp0 = torch.from_numpy(data['keypoints_start'].flatten().astype(np.float32)).cuda()\n    inp1 = torch.from_numpy(data['keypoints_goal_corrected'].flatten().astype(np.float32)).cuda()\n    return torch.cat((inp0, inp1))\n\ndef to_torch_trans(data):\n    inp0 = torch.from_numpy(data['keypoints_start'].flatten().astype(np.float32)).cuda()\n    return inp0\n\ndef to_torch_trans_full(data):\n    inp0 = torch.from_numpy(data['keypoints_start'].flatten().astype(np.float32)).cuda()\n    inp1 = torch.from_numpy(np.asarray(data['transformation_corrected'], dtype=np.float32)).cuda()\n    return inp0, inp1\n\ndef eval_grasp_trans_sample(keypoints_start, trans_sample, grasp_vae, grasp_clf):\n    inp = torch.cat((keypoints_start, trans_sample))\n    z_sample = grasp_vae.encode(inp)[1]\n    \n    log_prob = grasp_clf.score(z_sample.data.cpu().numpy().reshape(1, -1))\n    return log_prob\n\ndef eval_contact_sample(keypoints_start, keypoints_goal, palm_sample, vae, clf):\n    inp = torch.cat((keypoints_start, keypoints_goal, palm_sample))\n    z_sample = vae.encode(inp)[1]\n    \n    log_prob = clf.score(z_sample.data.cpu().numpy().reshape(1, -1))\n    return log_prob\n\ndef empty_state():\n    data = {}\n    data['keypoints_start'] = None\n    data['keypoints_goal_corrected'] = []\n    data['transformation_corrected'] = []\n    data['start'] = None\n    data['goal'] = None\n    data['contact_obj_frame'] = {}\n    data['contact_obj_frame']['right'] = None\n    data['contact_obj_frame']['left'] = None\n    return data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def sample_contact(vae, start_data, primitive='pull', num_samples=1, clf=None):\n    output_data = copy.deepcopy(start_data)\n    output_data['contact_obj_frame']['right'] = []\n    output_data['contact_obj_frame']['left'] = []    \n    \n    x = to_torch_contact(start_data)\n    for i in range(num_samples):\n        if clf is None:\n            z_sample = torch.randn_like(torch.zeros(3,)).cuda()\n        else:\n            z_sample = torch.from_numpy(clf.sample()[0].squeeze()).cuda()\n\n        palm_poses = vae.decode(z_sample, x)\n        r_palm, l_palm = palm_poses[0], palm_poses[1]\n\n        output_data['contact_obj_frame']['right'].append(torch_pose_to_list(r_palm))\n        output_data['contact_obj_frame']['left'].append(torch_pose_to_list(l_palm))\n        \n    return output_data\n\ndef sample_pull_transformation(pull_vae, start_data, num_samples=1, pull_clf=None):\n    x = torch.from_numpy(start_data['keypoints_start'].flatten().astype(np.float32)).cuda()\n    \n    output_data = copy.deepcopy(start_data)\n    output_data['transformation_corrected'] = []\n    output_data['keypoints_goal_corrected'] = []    \n    for i in range(num_samples):\n        z_sample = torch.randn_like(torch.zeros(3,)).cuda()\n        trans = pull_vae.decode(z_sample, x)\n        \n        output_data['transformation_corrected'].append(torch_pose_to_list(trans))\n    \n    output_data = transform_keypoints(output_data, number=num_samples)\n    return output_data\n\ndef sample_grasp_transformation(grasp_vae, grasp_clf, start_data, num_samples=1, \n                                max_k=100, best_threshold=9.0, keep_best_only=False):\n    x = torch.from_numpy(start_data['keypoints_start'].flatten().astype(np.float32)).cuda()\n    \n    output_data = copy.deepcopy(start_data)\n    output_data['transformation_corrected'] = []\n    output_data['keypoints_goal_corrected'] = []\n    \n    best_trans_list = []\n    best_score_list = []\n    for i in range(num_samples):\n        k = 0\n        done = False\n        best_so_far = 0.0\n        best_trans = None\n        while not done:\n            k += 1\n            \n            z_sample = torch.randn_like(torch.zeros(3,)).cuda()\n#             z_sample = torch.from_numpy(grasp_clf.sample()[0].squeeze().astype(np.float32)).cuda()\n            trans = grasp_vae.decode(z_sample, x)\n            if best_trans is None:\n                best_trans = trans\n    \n            log_prob = eval_grasp_trans_sample(x, trans, grasp_vae, grasp_clf)\n            \n            if log_prob > best_so_far:\n                best_so_far = log_prob\n                best_trans = trans\n        \n            if best_so_far > best_threshold or k >= max_k:\n                break\n                \n        output_data['transformation_corrected'].append(torch_pose_to_list(trans))\n        best_trans_list.append(best_trans)\n        best_score_list.append(best_so_far)\n    if keep_best_only:\n        best_ind = np.argmax(best_score_list)\n        output_data['transformation_corrected'] = [output_data['transformation_corrected'][best_ind]]\n        output_data = transform_keypoints(output_data, number=1)\n    else:\n        output_data = transform_keypoints(output_data, number=num_samples)\n    return output_data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Load data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "grasp_loader = DataLoader(data_dir = '/root/catkin_ws/src/primitives/data/grasp/face_ind_test_0_fixed/train/')\ngrasp_loader.create_random_ordering()\n\npull_loader = DataLoader(data_dir = '/root/catkin_ws/src/primitives/data/pull/pull_face_all_0/train/')\npull_loader.create_random_ordering()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "pull_data = get_rand_data(primitive='pull', train_test='test')\n# vis_palms(pull_data, name='pull').show(viewer='gl')\npull_data_eval = copy.deepcopy(pull_data)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "grasp_data = get_rand_data(primitive='grasp', train_test='test')\n# vis_palms(grasp_data, name='grasp').show(viewer='gl')\ngrasp_data_eval = copy.deepcopy(grasp_data)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Load models and fit latent space PDF"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "grasp_palm_vae = VAE(48+14, 7, 3, 48, [512, 256, 128, 64], 0.0003)\nload_net_state(grasp_palm_vae, '/root/training/saved_models/grasp_contact_keypoints_start_goal_fixed_full_data_anneal_batch128_fixed/grasp_contact_keypoints_start_goal_fixed_full_data_anneal_batch128_fixed_epoch_100.pt')\n\ngrasp_palm_vae.encoder.cuda()\ngrasp_palm_vae.decoder.cuda()\n\n\ndataset = grasp_loader.load_dataset(start_rep='keypoints', goal_rep='keypoints', task='contact')\nx = torch.from_numpy(dataset[0]).cuda()\nz, z_mu, z_logvar = grasp_palm_vae.encode(x)\n\nx_train = z_mu.data.cpu().numpy()\n\ngrasp_palm_clf = GaussianMixture(n_components=10, covariance_type='full')\ngrasp_palm_clf.fit(x_train)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# grasp_palm_vae = VAE(48+14, 7, 3, 48, [512, 256, 128, 64], 0.0003)\n# load_net_state(grasp_palm_vae, '/root/training/saved_models/grasp_contact_keypoints_start_goal_fixed_full_data_anneal_batch128_fixed/grasp_contact_keypoints_start_goal_fixed_full_data_anneal_batch128_fixed_epoch_150.pt')\n\n# grasp_palm_vae.encoder.cuda()\n# grasp_palm_vae.decoder.cuda()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "pull_trans_vae = GoalVAE(31, 7, 3, 24, [512, 256, 128, 64], 0.0003)\n# load_net_state(pull_trans_vae, '/root/training/saved_models/pull_transformation_full_data_start_keypoints_cond_batch_512_fixed_1/pull_transformation_full_data_start_keypoints_cond_batch_512_fixed_1_epoch_50.pt')\n# load_net_state(pull_trans_vae, '/root/training/saved_models/pull_transformation_full_data_start_keypoints_cond_batch_512_fixed_xy_relax_0/pull_transformation_full_data_start_keypoints_cond_batch_512_fixed_xy_relax_0_epoch_150.pt')\nload_net_state(pull_trans_vae, '/root/training/saved_models/pull_transformation_full_data_start_keypoints_cond_batch_512_fixed_no_xy_0/pull_transformation_full_data_start_keypoints_cond_batch_512_fixed_no_xy_0_epoch_150.pt')\n\npull_trans_vae.encoder.cuda()\npull_trans_vae.decoder.cuda()\n\n\ndataset = pull_loader.load_dataset(start_rep='keypoints', goal_rep='keypoints', task='transformation')\nx = torch.from_numpy(dataset[0]).cuda()\nz, z_mu, z_logvar = pull_trans_vae.encode(x)\n\nx_train = z_mu.data.cpu().numpy()\n\npull_trans_clf = GaussianMixture(n_components=10, covariance_type='full')\npull_trans_clf.fit(x_train)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "grasp_trans_vae = GoalVAE(31, 7, 3, 24, [512, 256, 128, 64], 0.0003)\nload_net_state(grasp_trans_vae, '/root/training/saved_models/grasp_transformation_cond_start_full_data_fixed_anneal_0/grasp_transformation_cond_start_full_data_fixed_anneal_0_epoch_980.pt')\n\ngrasp_trans_vae.encoder.cuda()\ngrasp_trans_vae.decoder.cuda()\n\ndataset = grasp_loader.load_dataset(start_rep='keypoints', goal_rep='keypoints', task='transformation')\nx = torch.from_numpy(dataset[0]).cuda()\nz, z_mu, z_logvar = grasp_trans_vae.encode(x)\n\nx_train = z_mu.data.cpu().numpy()\n\ngrasp_trans_clf = GaussianMixture(n_components=10, covariance_type='full')\ngrasp_trans_clf.fit(x_train)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# pull_contact_out = sample_contact(pull_palm_vae, pull_data_eval, primitive='pull', num_samples=10)\n\n# vis_palms(pull_contact_out, name='pull', goal='transformation', palm_number=10).show(viewer='gl')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "grasp_contact_out = sample_contact(grasp_palm_vae, grasp_data_eval, primitive='grasp', num_samples=10)\n\nvis_palms(grasp_contact_out, name='grasp', goal='transformation', palm_number=10).show(viewer='gl')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "pull_data_0 = get_rand_data(primitive='pull', train_test='test')\n# vis_palms(pull_data_0, name='pull').show(viewer='gl')\n# plot_keypoint_goal(pull_data_0).show()\n# pull_data_eval_0 = copy.deepcopy(pull_data_0)\n\nout_pull = sample_pull_transformation(pull_trans_vae, pull_data_0, num_samples=20)\n\npull_fig = plot_keypoint_goal(out_pull)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "pull_fig.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "vis_palms(out_pull, name='pull', goal='transformation',goal_number=10).show(viewer='gl')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "out_grasp = sample_grasp_transformation(grasp_trans_vae, grasp_trans_clf, grasp_data_eval, num_samples=10)\n\ngrasp_fig = plot_keypoint_goal(out_grasp)\ngrasp_fig.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Planning sequences"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "pull_data_0 = get_rand_data(primitive='pull', train_test='test')\n# vis_palms(pull_data_0, name='pull').show(viewer='gl')\nplot_keypoint_goal(pull_data_0).show()\npull_data_eval_0 = copy.deepcopy(pull_data_0)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "pull_data_1 = get_rand_data(primitive='pull', train_test='test')\n# vis_palms(pull_data_1, name='pull').show(viewer='gl')\nplot_keypoint_goal(pull_data_1).show()\npull_data_eval_1 = copy.deepcopy(pull_data_1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "grasp_data = get_rand_data(primitive='grasp', train_test='test')\n# vis_palms(grasp_data, name='grasp').show(viewer='gl')\nplot_keypoint_goal(grasp_data).show()\ngrasp_data_eval = copy.deepcopy(grasp_data)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def at_goal(current, goal, tol=0.003):\n    center_current_xyz = np.mean(current['keypoints_start'], axis=0)\n    center_goal_xyz = np.mean(goal['keypoints_goal_corrected'], axis=0)\n    \n    dist = np.linalg.norm(center_current_xyz - center_goal_xyz)\n    if dist < tol:\n        return True, dist\n    else:\n        return False, dist"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "start_state = copy.deepcopy(pull_data_0)\ngoal_state = copy.deepcopy(pull_data_1)\ngrasp_goal = {}\ngrasp_goal['keypoints_goal_corrected'] = grasp_data_eval['keypoints_start']\nkeypoint_plan = []\nkeypoint_plan.append(start_state)\n\ndone = False\nk = 0\nmax_k = 30\nbest_dist = 1.0\nbest_score = -100.0\n\nnodes = 8\nexecuted_grasp = False\n\ncurrent_state = start_state\nwhile not done:\n    \n    # get new pull transformations from current\n    new_pull_state = sample_pull_transformation(\n        pull_vae=pull_trans_vae, \n        start_data=current_state,\n        num_samples=nodes)\n#     new_pull_state = transform_keypoints(new_pull_state, number=nodes)\n    new_pull_state = transform_keypoints_full(new_pull_state, number=nodes)\n    \n    # get new grasp transformations from current\n    new_grasp_state = sample_grasp_transformation(\n        grasp_vae=grasp_trans_vae, \n        grasp_clf=grasp_trans_clf,\n        start_data=current_state,\n        num_samples=nodes)\n#     new_grasp_state = transform_keypoints(new_grasp_state, number=nodes)\n    new_grasp_state = transform_keypoints_full(new_grasp_state, number=nodes)\n    new_grasp_state_scores = []\n    \n    # evaluate grasp sample from current\n#     keypoints_start, trans = to_torch_trans_full(new_grasp_state)\n#     for i in range(nodes):\n#         log_prob = eval_grasp_trans_sample(\n#             keypoints_start=keypoints_start, \n#             trans_sample=trans[i, :], \n#             grasp_vae=grasp_trans_vae, \n#             grasp_clf=grasp_trans_clf)\n#         new_grasp_state_scores.append(log_prob)\n        \n#     print(\"Scores from current: \")\n#     for i, score in enumerate(new_grasp_state_scores):\n#         print(i, score)\n\n    # sample grasps for each node expanded from pulling\n    grasp_states_post_pull = []\n    grasp_state_post_pull_dists = []\n    grasp_state_post_pull_log_probs = []\n    for i in range(nodes):\n        pull_state_expand = {}\n        pull_state_expand['keypoints_start'] = new_pull_state['keypoints_goal_corrected'][i]\n        pull_state_expand['transformation_corrected'] = [new_pull_state['transformation_corrected'][i]]\n        _, dist = at_goal(pull_state_expand, grasp_goal)\n\n        if dist < 0.05:\n            grasp_state_post_pull = sample_grasp_transformation(\n                grasp_vae=grasp_trans_vae,\n                grasp_clf=grasp_trans_clf,\n                start_data=pull_state_expand,\n                num_samples=1\n            )\n#             grasp_state_post_pull = transform_keypoints(grasp_state_post_pull)\n            grasp_state_post_pull = transform_keypoints_full(grasp_state_post_pull)\n\n            keypoints_start, trans = to_torch_trans_full(grasp_state_post_pull)\n\n            log_prob = eval_grasp_trans_sample(\n                keypoints_start=keypoints_start,\n                trans_sample=trans[0],\n                grasp_vae=grasp_trans_vae,\n                grasp_clf=grasp_trans_clf)\n            grasp_state_post_pull_log_probs.append(log_prob)\n        else:\n            grasp_state_post_pull_log_probs.append(-100.0)\n            grasp_state_post_pull = {}\n        \n        grasp_states_post_pull.append(grasp_state_post_pull)\n        grasp_state_post_pull_dists.append(dist)\n        \n    if len(grasp_state_post_pull_log_probs) > 0:\n        print(\"log probs after pulling: \")\n        closest_score_ind = np.argmax(grasp_state_post_pull_log_probs)\n        closest_score = grasp_state_post_pull_log_probs[closest_score_ind]        \n        for i, log_prob in enumerate(grasp_state_post_pull_log_probs):\n            if log_prob > 0.0:\n                print(i, log_prob)\n    else:\n        closest_score = -1.0\n\n#     print(\"dists after pulling: \")\n#     for i, dist in enumerate(grasp_state_post_pull_dists):\n#         print(i, dist)\n        \n    closest_dist_ind = np.argmin(grasp_state_post_pull_dists)    \n    closest_dist = grasp_state_post_pull_dists[closest_dist_ind]\n\n    prev_state = copy.deepcopy(current_state)\n    current_state = {}\n#     if closest_score > 11.0:\n    if closest_dist < 0.03 and closest_score > 11.0:\n#     if closest_score > 9.0:\n        print(\"executed grasp!\")\n        current_state['keypoints_start'] = grasp_states_post_pull[closest_dist_ind]['keypoints_goal_corrected'][0]\n        current_state['start'] = grasp_states_post_pull[closest_dist_ind]['goal'][0]\n#         current_state['keypoints_start'] = grasp_states_post_pull[closest_score_ind]['keypoints_goal_corrected'][0]\n#         current_state['start'] = grasp_states_post_pull[closest_score_ind]['goal'][0]\n        executed_grasp = True\n    else:\n        if closest_dist < best_dist:\n#         if closest_score > best_score:\n            best_dist = closest_dist\n#             best_score = closest_score\n            current_state['keypoints_start'] = new_pull_state['keypoints_goal_corrected'][closest_dist_ind]\n            current_state['start'] = new_pull_state['goal'][closest_dist_ind]\n#             current_state['keypoints_start'] = new_pull_state['keypoints_goal_corrected'][closest_score_ind]\n#             current_state['start'] = new_pull_state['goal'][closest_score_ind]\n        else:\n            current_state['keypoints_start'] = prev_state['keypoints_start']\n            current_state['start'] = prev_state['start']\n    keypoint_plan.append(current_state)\n    \n    close_to_goal, dist = at_goal(current_state, goal_state)\n    _, dist_grasp_goal = at_goal(current_state, grasp_goal)\n#     print(\"dist: \" + str(dist))\n    print(\"dist to grasp goal: \" + str(dist_grasp_goal))\n    \n    if close_to_goal or k > max_k or executed_grasp:\n        done = True\n    else:\n        k += 1\n    "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plan_fig = plot_keypoint_plan(keypoint_plan)\n\nplan_fig.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "mini_kp_plan = []\nmini_kp_plan.append(keypoint_plan[0])\nmini_kp_plan.append(keypoint_plan[5])\nmini_kp_plan.append(keypoint_plan[10])\nmini_kp_plan.append(keypoint_plan[15])\nmini_kp_plan.append(keypoint_plan[20])\n# copy.deepcopy(keypoint_plan)\n\nplan_fig = plot_keypoint_plan(mini_kp_plan)\n\nplan_fig.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "pull_test = sample_pull_transformation(start_data=keypoint_plan[-2], pull_vae=pull_trans_vae, num_samples=10)\npull_test = transform_keypoints(pull_test, number=10)\nplot_keypoint_goal(pull_test).show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "grasp_test = sample_grasp_transformation(start_data=keypoint_plan[-2], grasp_vae=grasp_trans_vae, grasp_clf=grasp_trans_clf, num_samples=10, keep_best_only=True)\n# grasp_test = transform_keypoints(grasp_test, number=1)\nplot_keypoint_goal(grasp_test).show()\n\n# vis_palms(grasp_test, name='grasp', goal='transformation', goal_number=10).show(viewer='gl')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "grasp_contact_test = empty_state()\nind = 3\ngrasp_contact_test['start'] = grasp_test['start']\ngrasp_contact_test['transformation_corrected'] = [grasp_test['transformation_corrected'][ind]]\ngrasp_contact_test['keypoints_start'] = grasp_test['keypoints_start']\ngrasp_contact_test['keypoints_goal_corrected'] = grasp_test['keypoints_goal_corrected'][ind]\n\ngrasp_contact_test_out = sample_contact(grasp_palm_vae, grasp_contact_test, primitive='grasp', num_samples=10)\n\nvis_palms(grasp_contact_test_out, name='grasp', goal='transformation', palm_number=10).show(viewer='gl')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "grasp_fig = plot_keypoint_goal(grasp_data_eval)\ngrasp_fig.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Analyze latent space"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "x_embedded = TSNE(n_components=2).fit_transform(x_train)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "x_sampled = grasp_trans_clf.sample(5000)[0]\n\nprint(x_sampled)\n\nprint(x_sampled.shape)\nprint(x_train.shape)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "x_sampled_embedded = TSNE(n_components=2).fit_transform(x_sampled)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plt.figure()\nplt.scatter(x_embedded[:, 0], x_embedded[:, 1], color='red')\nplt.scatter(x_sampled_embedded[:, 0], x_sampled_embedded[:, 1], color='blue')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Overfit to single batch"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "grasp_trans_vae = GoalVAE(31, 7, 3, 24, [64, 32], 0.0003)\nprint(grasp_trans_vae.encoder)\nprint(grasp_trans_vae.decoder)\nload_net_state(grasp_trans_vae, '/root/training/saved_models/grasp_transformation_cond_start_small_batch_small_model_2/grasp_transformation_cond_start_small_batch_small_model_2_epoch_140.pt')\ngrasp_trans_vae.encoder.cuda()\ngrasp_trans_vae.decoder.cuda()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "with open ('/root/catkin_ws/src/primitives/data/grasp/face_ind_test_0_fixed/train/4114/pkl/4114.pkl', 'rb') as f:\n    grasp_data_overfit = pickle.load(f)\n\ngrasp_data_overfit_copy = copy.deepcopy(grasp_data_overfit)    "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(grasp_data_overfit['transformation_corrected'])\n\nplot_keypoint_goal(transform_keypoints(grasp_data_overfit)).show()\n\nscene = vis_palms(grasp_data_overfit, name='grasp', goal='transformation')\nscene.show(viewer='gl')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "inp0 = torch.from_numpy(grasp_data_overfit['keypoints_start'].flatten().astype(np.float32)).cuda()\ninp1 = torch.from_numpy(np.asarray(grasp_data_overfit['transformation_corrected'], dtype=np.float32)).cuda()\ninp = torch.cat((inp0, inp1))\nz_sample = grasp_trans_vae.encode(inp)[1]\n\nx = torch.from_numpy(grasp_data_overfit['keypoints_start'].flatten().astype(np.float32)).cuda()\n\ntrans = grasp_trans_vae.decode(z_sample, x)\nprint(pose_to_list(trans))\n\ngrasp_data_overfit_copy['transformation_corrected'] = pose_to_list(trans)\nplot_keypoint_goal(transform_keypoints(grasp_data_overfit_copy)).show()\n\nscene = vis_palms(grasp_data_overfit_copy, name='grasp', goal='transformation')\nscene.show(viewer='gl')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(grasp_trans_vae.encoder)\nprint(grasp_trans_vae.decoder)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(grasp_trans_vae.encoder.hidden_layers)\nprint(grasp_trans_vae.decoder.hidden_layers)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "a = [64, 32]\nb = copy.deepcopy(a)\nb.reverse()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "grasp_data_overfit.keys()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(grasp_trans_vae.encode(inp)[1])\nprint(grasp_trans_vae.encoder.forward(inp))\n\nprint(grasp_trans_vae.decode(z_sample, x))\nprint(grasp_trans_vae.decoder.forward(torch.cat((z_sample, x))))\n\nprint(grasp_data_overfit['transformation_corrected'])\n\ntrans_inp = torch.from_numpy(np.asarray(grasp_data_overfit['transformation_corrected'], dtype=np.float32)).cuda()\nprint(grasp_trans_vae.mse(trans[:3], trans_inp[:3]))\n# print(trans_inp[3:].expand((2, 4)))\nprint(grasp_trans_vae.rotation_loss(trans[3:].expand(2, 4), trans_inp[3:].expand(2, 4)))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "grasp_trans_vae.decoder.state_dict().keys()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
