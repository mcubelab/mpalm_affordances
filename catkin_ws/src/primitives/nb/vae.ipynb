{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, sys\nsys.path.append('/root/catkin_ws/src/primitives/')\nimport pickle\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import axes3d\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport trimesh\nimport networkx\n\nfrom open3d import JVisualizer\n\nimport copy\nimport time\nimport argparse\nimport numpy as np\nfrom multiprocessing import Process, Pipe, Queue\nimport pickle\nimport rospy\nimport copy\nimport signal\nimport open3d\nfrom IPython import embed\n\nfrom yacs.config import CfgNode as CN\nfrom closed_loop_experiments import get_cfg_defaults\n\nfrom airobot import Robot\nfrom airobot.utils import pb_util\nfrom airobot.sensor.camera.rgbdcam_pybullet import RGBDCameraPybullet\nfrom airobot.utils import common\nimport pybullet as p\n\nfrom helper import util\nfrom macro_actions import ClosedLoopMacroActions, YumiGelslimPybulet\n# from closed_loop_eval import SingleArmPrimitives, DualArmPrimitives"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "sys.path.append('/root/training/')\n\nimport os\nimport argparse\nimport time\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.autograd import Variable\nfrom data_loader import DataLoader\nfrom model import VAE, GoalVAE\nfrom util import to_var, save_state, load_net_state, load_seed, load_opt_state\n\nimport scipy.signal as signal"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# with open ('/root/catkin_ws/src/primitives/data/grasp/face_ind_test_0/2.pkl', 'rb') as f:\n#     grasp_data = pickle.load(f)\n    \n# print(grasp_data.keys())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# with open ('/root/catkin_ws/src/primitives/data/pull/face_ind_large_0_fixed/1003.pkl', 'rb') as f:\n#     pull_data = pickle.load(f)\ndef get_rand_data(primitive='pull'):\n    got_file = False\n    while not got_file:\n        val = np.random.randint(low=0, high=5000)\n        if primitive == 'pull':\n            test_path = '/root/catkin_ws/src/primitives/data/pull/pull_face_all_0/test/%d.pkl' % val\n        elif primitive == 'grasp':\n            test_path = '/root/catkin_ws/src/primitives/data/grasp/face_ind_test_0_fixed/test/%d.pkl' % val\n        if os.path.exists(test_path):\n            with open (test_path, 'rb') as f:\n    #             pull_data = pickle.load(f)\n                data = pickle.load(f)\n                got_file = True\n\n    print(test_path)\n    # print(pull_data.keys())\n    print(data.keys())\n    return data\npull_data = get_rand_data(primitive='pull')\ngrasp_data = get_rand_data(primitive='grasp')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Training data: \n## Inputs: \nInitial observation, different representations:\n- ```'start'```: pose ```[x_pos, y_pos, z_pos, x_ori, y_ori, z_ori, w_ori]```\n- ```'keypoints_start'```: 3D location of box corners at start pose\n- ```'obs':'pcd_pts'```: Point cloud of box from 3 different viewpoints, with same global coordinate sys. Can be fused with np.concatenate (see below)\n\nGoal, different representations:\n- ```'goal'```: pose\n- ```'keypoints_goal'```: 3D location of box corners at goal pose"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# start observation\nstart = pull_data['start']\nkeypoints_start = pull_data['keypoints_start']\npcd_pts = pull_data['obs']['pcd_pts']\npcd_pts_start = np.concatenate(pcd_pts, axis=0)\n\n# goal\ngoal = pull_data['goal']\nkeypoints_goal = pull_data['keypoints_goal_corrected']\n\n# # start observation\n# start = grasp_data['start']\n# keypoints_start = grasp_data['keypoints_start']\n# pcd_pts = grasp_data['obs']['pcd_pts']\n# pcd_pts_start = np.concatenate(pcd_pts, axis=0)\n\n# # goal\n# goal = grasp_data['goal']\n# keypoints_goal = grasp_data['keypoints_goal']"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Outputs:\n## Pulling/Pushing (single arm)\nRobot palm pose in the object frame, for active arm -- active arm currently based on which side of the table the object starts on. TODO perhaps includes predicting which arm is active, once we move to more diverse data. For now, everything for pulling happens with the right arm\n- ```'contact_obj_frame'```: pose ```[x_pos, y_pos, z_pos, x_ori, y_ori, z_ori, w_ori]```, specified with respect to the coordinate system located at the object center of mass at the start pose\n\n## Grasping/Pivoting (dual arm)\n\nRight and left robot palm pose in the object frame\n- ```'contact_obj_frame':'right'```: right palm pose ```[x_pos, y_pos, z_pos, x_ori, y_ori, z_ori, w_ori]```, specified with respect to the coordinate system located at the object center of mass at the start pose\n- ```'contact_obj_frame':'left'```: left palm pose ```[x_pos, y_pos, z_pos, x_ori, y_ori, z_ori, w_ori]```, specified with respect to the coordinate system located at the object center of mass at the start pose"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# right palm contact for pulling\n# contact_r = pull_data['contact_obj_frame']['right']\n\n# both palms contact for grasping\ncontact_r = grasp_data['contact_obj_frame']['right']\ncontact_l = grasp_data['contact_obj_frame']['left']"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def pose_to_list(palm_pose_tensor):\n    pos = palm_pose_tensor[:3].data.cpu().numpy()\n    ori = palm_pose_tensor[3:].data.cpu().numpy()\n    \n    ori = ori/np.linalg.norm(ori)\n    \n    return pos.tolist() + ori.tolist()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Testing"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# load trained model\n\n# vae = VAE(14, 7, 2, 0.0003)\n# load_net_state(vae, '/root/training/saved_models/pose_init_small_batch_1/pose_init_small_batch_1_epoch_63.pt')\n\n# vae = VAE(31, 7, 3, 0.0003)\n# load_net_state(vae, '/root/training/saved_models/pull_keypoints_init_small_batch_0/pull_keypoints_init_small_batch_0_epoch_99.pt')\n\n# vae = VAE(14, 14, 3, 0.0003)\n# load_net_state(vae, '/root/training/saved_models/grasp_pose_init_small_batch_0/grasp_pose_init_small_batch_0_epoch_298.pt')\n\n# vae = VAE(31, 14, 3, 0.0003)\n# load_net_state(vae, '/root/training/saved_models/grasp_keypoints_init_small_batch_1/grasp_keypoints_init_small_batch_1_epoch_199.pt')\n\n# vae = VAE(31, 7, 3, 0.0003)\n# print(vae.encoder)\n# print(vae.decoder)\n# load_net_state(vae, '/root/training/saved_models/grasp_keypoints_two_heads_small_batch_r_only_0/grasp_keypoints_two_heads_small_batch_r_only_0_epoch_5.pt')\n\n# vae = VAE(31+14, 7, 3, 31, 0.0003)\n# print(vae.encoder)\n# print(vae.decoder)\n# load_net_state(vae, '/root/training/saved_models/grasp_keypoints_two_heads_full_data_grasp_input_1/grasp_keypoints_two_heads_full_data_grasp_input_1_epoch_199.pt')\n\n# vae = VAE(24+7+7, 7, 3, 24+7, 0.0003)\n# print(vae.encoder)\n# print(vae.decoder)\n# load_net_state(vae, '/root/training/saved_models/pull_keypoints_two_heads_diverse_goals_full_data_palm_input_0/pull_keypoints_two_heads_diverse_goals_full_data_palm_input_0_epoch_199.pt')\n\n\n# vae = GoalVAE(24+7, 7, 2, 24, 0.0003)\n# print(vae.encoder)\n# print(vae.decoder)\n# load_net_state(vae, '/root/training/saved_models/pull_keypoints_goal_small_batch_0/pull_keypoints_goal_small_batch_0_epoch_137.pt')\n\nvae = GoalVAE(7, 7, 4, 0, 0.0003)\nprint(vae.encoder)\nprint(vae.decoder)\nload_net_state(vae, '/root/training/saved_models/pull_transformation_small_batch_lr_1e2_ratio_test_1/pull_transformation_small_batch_lr_1e2_ratio_test_1_epoch_16.pt')\n\n\n# vae = GoalVAE(7, 7, 4, 0, 0.0003)\n# print(vae.encoder)\n# print(vae.decoder)\n# load_net_state(vae, '/root/training/saved_models/grasp_transformation_small_batch_lr_1e2_ratio_test_0/grasp_transformation_small_batch_lr_1e2_ratio_test_0_epoch_199.pt')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# sample from the latent space and use the decoder/generator to produce contacts, visualize below\n# x = torch.from_numpy(np.asarray(start+goal, dtype=np.float32))\n# x = torch.from_numpy(np.hstack((keypoints_start.flatten().astype(np.float32), np.asarray(goal, dtype=np.float32))))\n\nx = torch.from_numpy(np.hstack((keypoints_start.flatten().astype(np.float32), np.asarray(goal, dtype=np.float32))))\n\n# contact_input = torch.from_numpy(np.asarray(contact_r+contact_l, dtype=np.float32))\ncontact_input = torch.from_numpy(np.asarray(contact_r, dtype=np.float32))\n\ndecoder_input = x[:24+7]\nz, pose_sample, z_mu, z_logvar = output = vae.forward(torch.cat((x, contact_input), dim=0), decoder_input)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"mu: \" + str(z_mu) + \", std: \" + str(torch.exp(0.5*z_logvar)))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# pose = vae.decode(z)\n# pose = vae.decode(z_mu)\n\nz_sample = torch.randn_like(torch.zeros(3, ))\n\npose = vae.decode(z_sample, decoder_input)\nprint(pose)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# pose_r = pose[:7]\n# pose_l = pose[7:]\n\npose_r = pose[0]\npose_l = pose[1]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "pull_data_eval = copy.deepcopy(pull_data)\npull_data_eval['contact_obj_frame']['right'] = pose_to_list(pose_r)\n\n# grasp_data_eval = copy.deepcopy(grasp_data)\n# grasp_data_eval['contact_obj_frame']['right'] = pose_to_list(pose_r)\n# grasp_data_eval['contact_obj_frame']['left'] = pose_to_list(pose_l)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"data: \" + str(pull_data['contact_obj_frame']['right']))\nprint(\"--------\")\nprint(\"eval: \" + str(pull_data_eval['contact_obj_frame']['right']))\n\n# print(\"right: \" + str(grasp_data_eval['contact_obj_frame']['right']))\n# print(\"----------------------------\")\n# print(\"left: \" + str(grasp_data_eval['contact_obj_frame']['left']))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Visualization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "with open('/root/catkin_ws/src/primitives/data/pull/face_ind_large_0/metadata.pkl', 'rb') as mf:\n    metadata = pickle.load(mf)\n# with open('/root/catkin_ws/src/primitives/data/grasp/face_ind_test_0/metadata.pkl', 'rb') as mf:\n#     metadata = pickle.load(mf)\n\n\nprint('Metadata keys: ')\ndynamics_info = metadata['dynamics']\nmesh_file = metadata['mesh_file']\npalm_mesh_file = '/root/catkin_ws/src/config/descriptions/meshes/mpalm/mpalms_all_coarse.stl'\ntable_mesh_file = '/root/catkin_ws/src/config/descriptions/meshes/table/table_top.stl'\ncfg = metadata['cfg']"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Visualize contact on object"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def vis_palms(data, name='pull', goal='transformation'):\n    obj_mesh = trimesh.load_mesh(mesh_file)\n    r_palm_mesh = trimesh.load_mesh(palm_mesh_file)\n    l_palm_mesh = trimesh.load_mesh(palm_mesh_file)\n    table_mesh = trimesh.load_mesh(table_mesh_file)\n    \n    obj_pos_world = data['start'][:3]\n    obj_ori_world = data['start'][3:]\n    obj_ori_mat = common.quat2rot(obj_ori_world)\n    h_trans = np.zeros((4, 4))\n    h_trans[:3, :3] = obj_ori_mat\n    h_trans[:-1, -1] = obj_pos_world\n    h_trans[-1, -1] = 1\n\n    obj_mesh.apply_transform(h_trans)\n    \n    goal_obj_mesh = trimesh.load_mesh(mesh_file)\n    \n    if goal == 'transformation':\n        T_mat = util.matrix_from_pose(util.list2pose_stamped(data['transformation_corrected']))\n        T_start = util.matrix_from_pose(util.list2pose_stamped(data['start']))\n        T_goal = np.matmul(T_mat, T_start)\n        goal_obj_pose_world = util.pose_stamped2list(util.pose_from_matrix(T_goal))\n#         goal_obj_pose_start = util.list2pose_stamped(data['transformation_corrected'])\n#         goal_obj_pose_world = util.pose_stamped2list(\n#             util.convert_reference_frame(\n#                 goal_obj_pose_start,\n#                 util.unit_pose(),\n#                 util.list2pose_stamped(data['start'])))\n        goal_obj_pos_world = goal_obj_pose_world[:3]\n        goal_obj_ori_world = goal_obj_pose_world[3:]\n    else:\n        goal_obj_pos_world = data['goal'][:3]\n        goal_obj_ori_world = data['goal'][3:]\n    goal_obj_ori_mat = common.quat2rot(goal_obj_ori_world)\n    goal_h_trans = np.zeros((4, 4))\n    goal_h_trans[:3, :3] = goal_obj_ori_mat\n    goal_h_trans[:-1, -1] = goal_obj_pos_world\n    goal_h_trans[-1, -1] = 1 \n    \n    goal_obj_mesh.apply_transform(goal_h_trans)\n    \n    if name == 'pull':\n        tip_contact_r_obj = util.list2pose_stamped(data['contact_obj_frame']['right'])\n        tip_contact_r = util.convert_reference_frame(\n            pose_source=tip_contact_r_obj,\n            pose_frame_target=util.unit_pose(),\n            pose_frame_source=util.list2pose_stamped(data['start']))\n\n        wrist_contact_r = util.convert_reference_frame(\n            pose_source=util.list2pose_stamped(cfg.TIP_TO_WRIST_TF),\n            pose_frame_target=util.unit_pose(),\n            pose_frame_source=tip_contact_r)\n\n        wrist_contact_r_list = util.pose_stamped2list(wrist_contact_r)\n        \n        palm_pos_world_r = wrist_contact_r_list[:3]\n        palm_ori_world_r = wrist_contact_r_list[3:]\n        palm_ori_mat = common.quat2rot(palm_ori_world_r)\n        h_trans = np.zeros((4, 4))\n        h_trans[:3, :3] = palm_ori_mat\n        h_trans[:-1, -1] = palm_pos_world_r\n        h_trans[-1, -1] = 1\n\n        r_palm_mesh.apply_transform(h_trans)      \n        \n        scene = trimesh.Scene([obj_mesh, r_palm_mesh, table_mesh, goal_obj_mesh])        \n    else:\n        tip_contact_r_obj = util.list2pose_stamped(data['contact_obj_frame']['right'])\n        tip_contact_l_obj = util.list2pose_stamped(data['contact_obj_frame']['left'])\n\n        tip_contact_r = util.convert_reference_frame(\n            pose_source=tip_contact_r_obj,\n            pose_frame_target=util.unit_pose(),\n            pose_frame_source=util.list2pose_stamped(data['start']))\n            \n        tip_contact_l = util.convert_reference_frame(\n            pose_source=tip_contact_l_obj,\n            pose_frame_target=util.unit_pose(),\n            pose_frame_source=util.list2pose_stamped(data['start']))            \n            \n        wrist_contact_r = util.convert_reference_frame(\n            pose_source=util.list2pose_stamped(cfg.TIP_TO_WRIST_TF),\n            pose_frame_target=util.unit_pose(),\n            pose_frame_source=tip_contact_r)\n\n        wrist_contact_l = util.convert_reference_frame(\n            pose_source=util.list2pose_stamped(cfg.TIP_TO_WRIST_TF),\n            pose_frame_target=util.unit_pose(),\n            pose_frame_source=tip_contact_l)\n\n        wrist_contact_r_list = util.pose_stamped2list(wrist_contact_r)\n        wrist_contact_l_list = util.pose_stamped2list(wrist_contact_l)\n        \n        palm_pos_world_r = wrist_contact_r_list[:3]\n        palm_ori_world_r = wrist_contact_r_list[3:]\n        palm_ori_mat = common.quat2rot(palm_ori_world_r)\n        h_trans = np.zeros((4, 4))\n        h_trans[:3, :3] = palm_ori_mat\n        h_trans[:-1, -1] = palm_pos_world_r\n        h_trans[-1, -1] = 1\n\n        r_palm_mesh.apply_transform(h_trans)\n        \n        palm_pos_world_l = wrist_contact_l_list[:3]\n        palm_ori_world_l = wrist_contact_l_list[3:]\n        palm_ori_mat = common.quat2rot(palm_ori_world_l)\n        h_trans = np.zeros((4, 4))\n        h_trans[:3, :3] = palm_ori_mat\n        h_trans[:-1, -1] = palm_pos_world_l\n        h_trans[-1, -1] = 1\n\n        l_palm_mesh.apply_transform(h_trans)        \n        \n\n        scene = trimesh.Scene([obj_mesh, r_palm_mesh, l_palm_mesh, table_mesh, goal_obj_mesh])\n#         scene = trimesh.Scene([obj_mesh, r_palm_mesh, table_mesh])\n    good_camera_euler = [1.0513555,  -0.02236318, -1.62958927]\n    box_count = 0\n    for key in scene.geometry.keys():\n        print(key)\n        if 'mpalms_all_coarse' in key:\n            scene.geometry[key].visual.face_colors = [100, 100, 0, 30]\n        if 'realsense_box_experiments' in key:\n            if box_count == 0:\n                scene.geometry[key].visual.face_colors = [250, 200, 200, 150]\n                box_count += 1\n            else:\n                scene.geometry[key].visual.face_colors = [200, 200, 250, 150]                \n\n    scene.geometry['table_top.stl'].visual.face_colors = [200, 200, 200, 250]\n#     scene.geometry['realsense_box_experiments.stl'].visual.face_colors = [200, 200, 250, 250]\n    scene.set_camera(angles=good_camera_euler, center=data['start'][:3], distance=0.8)\n    return scene"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def correct_pos(data):\n    contact_obj_frame_pred = util.list2pose_stamped(pull_data_eval['contact_obj_frame'])\n    contact_world_frame = util.convert_reference_frame(contact_obj_frame_pred, util.unit_pose(), util.list2pose_stamped(start))\n    contact_pos = open3d.utility.DoubleVector(np.array(util.pose_stamped2list(contact_world_frame)[:3]))\n\n    pcd = open3d.geometry.PointCloud()\n    pcd.points = open3d.utility.Vector3dVector(np.concatenate(pull_data_eval['obs']['pcd_pts']))\n    pcd.colors = open3d.utility.Vector3dVector(np.concatenate(pull_data_eval['obs']['pcd_colors']) / 255.0)\n\n    kdtree = open3d.geometry.KDTreeFlann(pcd)\n    nearest_pt_ind = kdtree.search_knn_vector_3d(contact_pos, 1)[1][0]\n    nearest_pt_world = np.asarray(pcd.points)[nearest_pt_ind]\n\n    contact_world_frame_corrected = copy.deepcopy(contact_world_frame)\n    contact_world_frame_corrected.pose.position.x = nearest_pt_world[0]\n    contact_world_frame_corrected.pose.position.y = nearest_pt_world[1]\n    contact_world_frame_corrected.pose.position.z = nearest_pt_world[2]\n\n    contact_obj_frame_corrected = util.pose_stamped2list(util.convert_reference_frame(contact_world_frame_corrected, util.list2pose_stamped(start), util.unit_pose()))\n    new_data = copy.deepcopy(data)\n    new_data['contact_obj_frame'] = contact_obj_frame_corrected\n    return new_data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# scene = vis_palms(grasp_data, name='grasp', goal='goal')\nscene = vis_palms(get_rand_data('grasp'), name='grasp', goal='transformation')\n# scene = vis_palms(pull_data, name='pull', goal='transformation')\n# scene = vis_palms(get_rand_data('pull'), name='pull', goal='transformation')\n\nscene.show(viewer='gl')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "z_sample = torch.randn_like(torch.zeros(3, ))\n\nx = torch.from_numpy(np.hstack((keypoints_start.flatten().astype(np.float32), np.asarray(goal, dtype=np.float32))))\ndecoder_input = x[:31]\n\npose = vae.decode(z_sample, decoder_input)\npose_r = pose[0]\npose_l = pose[1]\n\npull_data_eval = copy.deepcopy(pull_data)\npull_data_eval['contact_obj_frame']['right'] = pose_to_list(pose_r)\npull_data_eval['contact_obj_frame']['left'] = pose_to_list(pose_l)\n\nscene = vis_palms(pull_data_eval, name='pull')\n\n# grasp_data_eval = copy.deepcopy(grasp_data)\n# grasp_data_eval['contact_obj_frame']['right'] = pose_to_list(pose_r)\n# grasp_data_eval['contact_obj_frame']['left'] = pose_to_list(pose_l)\n\n# scene = vis_palms(grasp_data_eval, name='grasp')\nscene.show(viewer='gl')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# scene = vis_palms(pull_data_eval, name='pull')\nscene = vis_palms(grasp_data_eval, name='grasp')\n\nscene.show(viewer='gl')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# scene = vis_palms(correct_pos(pull_data_eval), name='pull')\n# scene = vis_palms(grasp_data, name='grasp')\n\n# scene.show(viewer='gl')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# x = torch.from_numpy(np.asarray(start+goal, dtype=np.float32))\n# output = vae.forward(x)\n\n# pos = output[1][:3].data.cpu().numpy()\n# ori = output[1][3:].data.cpu().numpy()\n\n# ori = ori/np.linalg.norm(ori)\n\n# pull_data_eval = copy.deepcopy(pull_data)\n# pull_data_eval['contact_obj_frame'] = pos.tolist() + ori.tolist()\n\n# scene = vis_palms(pull_data_eval, name='pull')\n\n# x = torch.from_numpy(np.hstack((keypoints_start.flatten().astype(np.float32), np.asarray(goal, dtype=np.float32))))\n# output = vae.forward(x)\n\n# pose_r = output[1][:7]\n# pose_l = output[1][7:]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "outputs = []\nz = torch.randn_like(torch.zeros(500, 2))\ndecoder_inputs = decoder_input.repeat(500).reshape(500, decoder_input.shape[0])\n\noutput = vae.decode(z, decoder_inputs)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# output_mean = []\n# output_std = []\n# for i in range(output[0].shape[1]):\n#     avg_dim_i = torch.mean(output[0][:, i], axis=0)\n#     std_dim_i = torch.std(output[0][:, i], axis=0)\n#     print(avg_dim_i)\n#     output_mean.append(avg_dim_i.data.cpu())\n#     output_std.append(std_dim_i.data.cpu())\n\noutput_mean = []\noutput_std = []\nfor i in range(output[0].shape[1]):\n    avg_dim_i_r = torch.mean(output[0][:, i], axis=0)\n    std_dim_i_r = torch.std(output[0][:, i], axis=0)\n    \n    avg_dim_i_l = torch.mean(output[1][:, i], axis=0)\n    std_dim_i_l = torch.std(output[1][:, i], axis=0)    \n    print(avg_dim_i)\n    output_mean.append((avg_dim_i_r.data.cpu(), avg_dim_i_l.data.cpu()))\n    output_std.append((std_dim_i_r.data.cpu(), avg_dim_i_l.data.cpu()))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# pos = np.asarray(output_mean[:3])\n# ori = np.asarray(output_mean[3:])\n\n# ori = ori/np.linalg.norm(ori)\n\n# pull_data_eval = copy.deepcopy(pull_data)\n# pull_data_eval['contact_obj_frame'] = pos.tolist() + ori.tolist()\n\n# scene = vis_palms(pull_data_eval, name='pull')\n\n# pose_r = output_mean[:7]\n# pose_l = output_mean[7:]\n\n# pose_r = output_mean[0][:7]\n# pose_l = output_mean[1][7:]\n\npose_r, pose_l = [], []\nfor i in range(output[0].shape[1]):\n    pose_r.append(output_mean[i][0])\n    pose_l.append(output_mean[i][1])\n\ngrasp_data_eval = copy.deepcopy(grasp_data)\ngrasp_data_eval['contact_obj_frame']['right'] = pose_r\ngrasp_data_eval['contact_obj_frame']['left'] = pose_l\nscene = vis_palms(grasp_data_eval, name='grasp')\n\nscene.show(viewer='gl')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "loss_data = np.load('/root/training/saved_models/grasp_keypoints_two_heads_full_data_grasp_input_1/grasp_keypoints_two_heads_full_data_grasp_input_1_epoch_199_recon_loss.npz', allow_pickle=True)\nloss_data.keys()\nloss_data['pos_loss'].shape"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plt.figure()\nplt.plot(loss_data['pos_loss'])\nplt.show()\nplt.title('Position loss')\nplt.ylabel('Loss')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plt.figure()\nplt.plot(loss_data['ori_loss'])\nplt.show()\nplt.title('Orientation loss')\nplt.ylabel('Loss')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# sos = signal.butter(3, 0.03, 'low', output='sos')\nsos = signal.butter(3, 0.2, 'low', output='sos')\nfiltered = signal.sosfilt(sos, loss_data['ori_loss'])\n\nplt.figure()\nplt.plot(filtered[4:])\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Testing setting goals"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "x = torch.from_numpy(keypoints_start.flatten().astype(np.float32))\nx.shape"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# contact_input = torch.from_numpy(np.asarray(contact_r+contact_l, dtype=np.float32))\n# contact_input = torch.from_numpy(np.asarray(contact_r, dtype=np.float32))\n\ndecoder_input = goal\nz, pose_sample, z_mu, z_logvar = output = vae.forward(x, decoder_input)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "z_sample = torch.randn_like(torch.zeros(2, ))\n\nx = torch.from_numpy(keypoints_start.flatten().astype(np.float32))\n\npose = vae.decode(z_sample, x)\npose_object = pose\n\n# pull_data_eval = copy.deepcopy(pull_data)\npull_data_eval = copy.deepcopy(pull_data)\n# pull_data_eval['contact_obj_frame']['right'] = pose_to_list(pose_r)\n# pull_data_eval['contact_obj_frame']['left'] = pose_to_list(pose_l)\npull_data_eval['goal'] = pose_to_list(pose_object)\n\nscene = vis_palms(pull_data_eval, name='pull')\n\n# grasp_data_eval = copy.deepcopy(grasp_data)\n# grasp_data_eval['contact_obj_frame']['right'] = pose_to_list(pose_r)\n# grasp_data_eval['contact_obj_frame']['left'] = pose_to_list(pose_l)\n\n# scene = vis_palms(grasp_data_eval, name='grasp')\nscene.show(viewer='gl')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# grasp_data_eval = get_rand_data('grasp')\n# scene = vis_palms(grasp_data_eval, name='grasp', goal='transformation')\n\npull_data_eval = get_rand_data('pull')\nscene = vis_palms(pull_data_eval, name='pull', goal='transformation')\n\nscene.show(viewer='gl')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Test transformations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "z_sample = torch.randn_like(torch.zeros(4, ))\n\n# x = torch.from_numpy(keypoints_start.flatten().astype(np.float32))\n\ntrans = vae.decode(z_sample)\n\n# # pull_data_eval = copy.deepcopy(pull_data)\n# # pull_data_eval['contact_obj_frame']['right'] = pose_to_list(pose_r)\n# # pull_data_eval['contact_obj_frame']['left'] = pose_to_list(pose_l)\n# pull_data_eval = copy.deepcopy(pull_data)\n\npull_data_eval['transformation_corrected'] = pose_to_list(trans)\nscene = vis_palms(pull_data_eval, name='pull', goal='transformation')\n\n# grasp_data_eval = copy.deepcopy(grasp_data)\n# grasp_data_eval['contact_obj_frame']['right'] = pose_to_list(pose_r)\n# grasp_data_eval['contact_obj_frame']['left'] = pose_to_list(pose_l)\n\n# grasp_data_eval['transformation_corrected'] = pose_to_list(trans)\n# scene = vis_palms(grasp_data_eval, name='grasp', goal='transformation')\n\nscene.show(viewer='gl')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "total_loss = np.load('/root/training/saved_models/grasp_transformation_small_batch_lr_1e2_ratio_test_0/grasp_transformation_small_batch_lr_1e2_ratio_test_0_epoch_199_loss.npz', allow_pickle=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "total_loss['loss'][-1].keys()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "kl = []\npos = []\nori = []\nrecon = []\nfor i, val in enumerate(total_loss['loss']):\n    kl.append(val['epoch_kl'])\n    pos.append(val['epoch_pos'])\n    ori.append(val['epoch_ori'])\n    recon.append(val['epoch_recon'])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%matplotlib notebook"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "plt.subplot(211)\nplt.plot(np.asarray(kl))\nplt.legend(['kl'])\nplt.subplot(212)\nplt.plot(np.asarray(pos))\nplt.plot(np.asarray(ori))\nplt.legend(['pos', 'ori'])\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(np.argmin(pos))\nprint(np.argmin(ori))\nprint(np.argmin(recon))\nprint(np.argmin(kl))"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
